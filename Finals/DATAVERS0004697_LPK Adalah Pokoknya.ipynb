{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39089149",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e45114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification\n",
    ")\n",
    "from datasets import Dataset\n",
    "from seqeval.metrics import f1_score, precision_score, recall_score, classification_report\n",
    "import torch\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01628230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data from JSONL file\n",
    "train_data = []\n",
    "with open('/root/train.jsonl', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        train_data.append(json.loads(line))\n",
    "\n",
    "print(f\"Total training documents: {len(train_data)}\")\n",
    "print(f\"\\nSample document:\")\n",
    "print(json.dumps(train_data[0], indent=2)[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4d78bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "def audit_labels_memory(data):\n",
    "    print(\"Sedang menganalisis konsistensi label di memori...\")\n",
    "    \n",
    "    # Penyimpanan Data\n",
    "    text_label_counts = defaultdict(Counter) # {text: {label1: count, label2: count}}\n",
    "    suspicious_entries = [] # List untuk menyimpan data aneh berdasarkan pola\n",
    "    \n",
    "    # Statistik\n",
    "    total_ents = 0\n",
    "    \n",
    "    # 1. SCANNING DATA (Langsung dari variable train_data)\n",
    "    for doc in data:\n",
    "        doc_id = doc['document_id']\n",
    "        \n",
    "        for ent in doc['entities']:\n",
    "            text = ent['text']\n",
    "            label = ent['label']\n",
    "            total_ents += 1\n",
    "            \n",
    "            # Catat statistik ambiguitas (Teks sama tapi label beda)\n",
    "            text_label_counts[text][label] += 1\n",
    "            \n",
    "            # --- Cek Pelanggaran Pola (Heuristics) ---\n",
    "            \n",
    "            # a. Cek Email (Wajib ada @)\n",
    "            if label == 'EMAIL' and '@' not in text:\n",
    "                suspicious_entries.append({\n",
    "                    'Text': text, 'Label': label, 'Issue': 'Email tanpa \"@\"', 'Doc_ID': doc_id\n",
    "                })\n",
    "            \n",
    "            # b. Cek Phone (Wajib ada angka)\n",
    "            elif label == 'PHONE' and not any(c.isdigit() for c in text):\n",
    "                suspicious_entries.append({\n",
    "                    'Text': text, 'Label': label, 'Issue': 'Phone tanpa angka', 'Doc_ID': doc_id\n",
    "                })\n",
    "                \n",
    "            # c. Cek Username (Tidak boleh ada spasi, kecuali kasus khusus)\n",
    "            elif label == 'USERNAME' and ' ' in text.strip():\n",
    "                suspicious_entries.append({\n",
    "                    'Text': text, 'Label': label, 'Issue': 'Username berspasi', 'Doc_ID': doc_id\n",
    "                })\n",
    "                \n",
    "            # d. Cek Job Title (Diawali gelar seperti Mr/Mrs -> Potensi salah label jadi JOB padahal NAME)\n",
    "            elif label == 'JOB_TITLE' and re.match(r'^(Mr\\.|Mrs\\.|Ms\\.|Dr\\.)', text):\n",
    "                suspicious_entries.append({\n",
    "                    'Text': text, 'Label': label, 'Issue': 'Job Title diawali Gelar (Potensi Nama)', 'Doc_ID': doc_id\n",
    "                })\n",
    "\n",
    "            # e. Cek Name (Jika isinya angka saja -> Potensi Phone/Date)\n",
    "            elif label == 'NAME' and re.match(r'^\\d+$', text):\n",
    "                 suspicious_entries.append({\n",
    "                    'Text': text, 'Label': label, 'Issue': 'Nama berupa Angka', 'Doc_ID': doc_id\n",
    "                })\n",
    "\n",
    "    # 2. PROSES AMBIGUITAS\n",
    "    ambiguous_data = []\n",
    "    for text, counts in text_label_counts.items():\n",
    "        if len(counts) > 1: # Punya lebih dari 1 jenis label\n",
    "            conflict_detail = \", \".join([f\"{lbl} ({cnt})\" for lbl, cnt in counts.items()])\n",
    "            total_cases = sum(counts.values())\n",
    "            ambiguous_data.append({\n",
    "                'Text': text,\n",
    "                'Conflicts': conflict_detail,\n",
    "                'Total_Count': total_cases\n",
    "            })\n",
    "    \n",
    "    # 3. MENAMPILKAN HASIL\n",
    "    \n",
    "    # A. Laporan Ambiguitas\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"ðŸš© LAPORAN 1: AMBIGUITAS LABEL (Top 15 Kasus)\")\n",
    "    print(\"Teks yang SAMA persis tapi dilabeli BERBEDA di dokumen lain.\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if ambiguous_data:\n",
    "        df_ambig = pd.DataFrame(ambiguous_data).sort_values('Total_Count', ascending=False)\n",
    "        # Tampilkan tabel rapi\n",
    "        display(df_ambig.head(15))\n",
    "    else:\n",
    "        print(\"âœ… Tidak ditemukan ambiguitas label.\")\n",
    "\n",
    "    # B. Laporan Data Mencurigakan\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"ðŸš© LAPORAN 2: DATA MENCURIGAKAN (Heuristics Check)\")\n",
    "    print(\"Label yang melanggar aturan format umum.\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if suspicious_entries:\n",
    "        df_suspicious = pd.DataFrame(suspicious_entries)\n",
    "        \n",
    "        # Grouping berdasarkan Issue untuk ringkasan\n",
    "        for issue in df_suspicious['Issue'].unique():\n",
    "            subset = df_suspicious[df_suspicious['Issue'] == issue]\n",
    "            print(f\"\\n--- Masalah: {issue} (Total: {len(subset)}) ---\")\n",
    "            display(subset[['Text', 'Label', 'Doc_ID']].head(5))\n",
    "    else:\n",
    "        print(\"âœ… Tidak ditemukan data yang melanggar pola umum.\")\n",
    "\n",
    "# Jalankan Fungsi menggunakan variabel train_data yang sudah ada\n",
    "audit_labels_memory(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d8c394",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers datasets accelerate seqeval -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ca2078",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification\n",
    ")\n",
    "from datasets import Dataset\n",
    "from seqeval.metrics import f1_score, precision_score, recall_score, classification_report\n",
    "import torch\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Label Schema and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define PII labels using BIO tagging scheme\n",
    "# B- = Beginning of entity, I- = Inside entity, O = Outside entity\n",
    "labels = [\n",
    "    \"O\",\n",
    "    \"B-NAME\", \"I-NAME\",\n",
    "    \"B-DATE\", \"I-DATE\",\n",
    "    \"B-EMAIL\", \"I-EMAIL\",\n",
    "    \"B-PHONE\", \"I-PHONE\",\n",
    "    \"B-JOB_TITLE\", \"I-JOB_TITLE\",\n",
    "    \"B-ADDRESS\", \"I-ADDRESS\",\n",
    "    \"B-USERNAME\", \"I-USERNAME\"\n",
    "]\n",
    "\n",
    "label2id = {label: i for i, label in enumerate(labels)}\n",
    "id2label = {i: label for i, label in enumerate(labels)}\n",
    "\n",
    "print(f\"Number of labels: {len(labels)}\")\n",
    "print(f\"Labels: {labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load training data from JSONL file\n",
    "train_data = []\n",
    "with open('/root/train.jsonl', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        train_data.append(json.loads(line))\n",
    "\n",
    "print(f\"Total training documents: {len(train_data)}\")\n",
    "print(f\"\\nSample document:\")\n",
    "print(json.dumps(train_data[0], indent=2)[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prepare Data for Token Classification\n",
    "\n",
    "We need to convert character-level annotations to token-level annotations that BERT can understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_character_labels(text, entities):\n",
    "    \"\"\"Create character-level labels for the text\"\"\"\n",
    "    char_labels = ['O'] * len(text)\n",
    "    \n",
    "    for entity in entities:\n",
    "        start = entity['start']\n",
    "        end = entity['end']\n",
    "        label = entity['label']\n",
    "        \n",
    "        # First character gets B- (Beginning)\n",
    "        char_labels[start] = f'B-{label}'\n",
    "        \n",
    "        # Remaining characters get I- (Inside)\n",
    "        for i in range(start + 1, end):\n",
    "            char_labels[i] = f'I-{label}'\n",
    "    \n",
    "    return char_labels\n",
    "\n",
    "# Test the function\n",
    "sample_text = train_data[0]['full_text']\n",
    "sample_entities = train_data[0]['entities']\n",
    "char_labels = create_character_labels(sample_text, sample_entities)\n",
    "\n",
    "print(f\"Text length: {len(sample_text)}\")\n",
    "print(f\"Labels length: {len(char_labels)}\")\n",
    "print(f\"\\nSample entities found: {len([l for l in char_labels if l != 'O'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Initialize Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize BERT tokenizer\n",
    "MODEL_NAME = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(f\"Tokenizer loaded: {MODEL_NAME}\")\n",
    "print(f\"Vocab size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Tokenization and Label Alignment\n",
    "\n",
    "We need to align character-level labels with token-level labels for BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(text, entities, tokenizer, max_length=512):\n",
    "    \"\"\"\n",
    "    Tokenize text and align labels from character-level to token-level\n",
    "    \"\"\"\n",
    "    # Create character-level labels\n",
    "    char_labels = create_character_labels(text, entities)\n",
    "    \n",
    "    # Tokenize with offset mapping to track character positions\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=False,\n",
    "        return_offsets_mapping=True\n",
    "    )\n",
    "    \n",
    "    # Align labels to tokens\n",
    "    token_labels = []\n",
    "    offset_mapping = encoding['offset_mapping']\n",
    "    \n",
    "    for start_char, end_char in offset_mapping:\n",
    "        # Special tokens (like [CLS], [SEP]) have offset (0, 0)\n",
    "        if start_char == 0 and end_char == 0:\n",
    "            token_labels.append(-100)  # Ignore special tokens in loss\n",
    "        else:\n",
    "            # Use the label of the first character of the token\n",
    "            token_labels.append(label2id[char_labels[start_char]])\n",
    "    \n",
    "    # Remove offset_mapping as it's not needed for training\n",
    "    encoding.pop('offset_mapping')\n",
    "    encoding['labels'] = token_labels\n",
    "    \n",
    "    return encoding\n",
    "\n",
    "# Test tokenization\n",
    "sample_encoding = tokenize_and_align_labels(\n",
    "    train_data[0]['full_text'], \n",
    "    train_data[0]['entities'], \n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "print(f\"Input IDs shape: {len(sample_encoding['input_ids'])}\")\n",
    "print(f\"Labels shape: {len(sample_encoding['labels'])}\")\n",
    "print(f\"Sample tokens: {tokenizer.convert_ids_to_tokens(sample_encoding['input_ids'][:20])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Prepare Dataset\n",
    "\n",
    "Convert all training data to tokenized format and create train/validation split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tokenize all training data\n",
    "print(\"Tokenizing training data...\")\n",
    "tokenized_data = []\n",
    "\n",
    "for i, doc in enumerate(train_data):\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"Processing document {i}/{len(train_data)}\")\n",
    "    \n",
    "    encoding = tokenize_and_align_labels(\n",
    "        doc['full_text'],\n",
    "        doc['entities'],\n",
    "        tokenizer,\n",
    "        max_length=512\n",
    "    )\n",
    "    \n",
    "    tokenized_data.append(encoding)\n",
    "\n",
    "print(f\"\\nTokenized {len(tokenized_data)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create train/validation split (90/10)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_encodings, val_encodings = train_test_split(\n",
    "    tokenized_data, \n",
    "    test_size=0.1, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Convert to HuggingFace Dataset format\n",
    "train_dataset = Dataset.from_dict({\n",
    "    'input_ids': [enc['input_ids'] for enc in train_encodings],\n",
    "    'attention_mask': [enc['attention_mask'] for enc in train_encodings],\n",
    "    'labels': [enc['labels'] for enc in train_encodings]\n",
    "})\n",
    "\n",
    "val_dataset = Dataset.from_dict({\n",
    "    'input_ids': [enc['input_ids'] for enc in val_encodings],\n",
    "    'attention_mask': [enc['attention_mask'] for enc in val_encodings],\n",
    "    'labels': [enc['labels'] for enc in val_encodings]\n",
    "})\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize BERT model for token classification\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "print(f\"Model loaded: {MODEL_NAME}\")\n",
    "print(f\"Number of parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Define Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute micro F1, precision, and recall scores\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    \n",
    "    # Remove ignored index (special tokens)\n",
    "    true_labels = []\n",
    "    true_predictions = []\n",
    "    \n",
    "    for prediction, label in zip(predictions, labels):\n",
    "        true_label = []\n",
    "        true_prediction = []\n",
    "        \n",
    "        for pred_id, label_id in zip(prediction, label):\n",
    "            if label_id != -100:\n",
    "                true_label.append(id2label[label_id])\n",
    "                true_prediction.append(id2label[pred_id])\n",
    "        \n",
    "        true_labels.append(true_label)\n",
    "        true_predictions.append(true_prediction)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    results = {\n",
    "        'precision': precision_score(true_labels, true_predictions),\n",
    "        'recall': recall_score(true_labels, true_predictions),\n",
    "        'f1': f1_score(true_labels, true_predictions),\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Metrics function defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Configure Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Configure training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",  # Disable wandb/tensorboard\n",
    "    fp16=torch.cuda.is_available(),  # Use mixed precision if GPU available\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data collator for dynamic padding\n",
    "data_collator = DataCollatorForTokenClassification(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=True\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Evaluate on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"Validation Results:\")\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"  {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load test data\n",
    "test_data = []\n",
    "with open('/root/test.jsonl', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        test_data.append(json.loads(line))\n",
    "\n",
    "print(f\"Total test documents: {len(test_data)}\")\n",
    "print(f\"Sample test document ID: {test_data[0]['document_id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Define Prediction Function\n",
    "\n",
    "Convert model predictions back to character-level offsets for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def predict_entities(text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Predict PII entities in text and return character-level offsets\n",
    "    \"\"\"\n",
    "    # Tokenize\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_offsets_mapping=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    offset_mapping = encoding.pop('offset_mapping').squeeze().tolist()\n",
    "    \n",
    "    # Move to same device as model\n",
    "    device = model.device\n",
    "    encoding = {k: v.to(device) for k, v in encoding.items()}\n",
    "    \n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoding)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1).squeeze().tolist()\n",
    "    \n",
    "    # Ensure predictions is a list\n",
    "    if not isinstance(predictions, list):\n",
    "        predictions = [predictions]\n",
    "    \n",
    "    # Convert token predictions to entities\n",
    "    entities = []\n",
    "    current_entity = None\n",
    "    \n",
    "    for idx, (pred_id, (start_char, end_char)) in enumerate(zip(predictions, offset_mapping)):\n",
    "        # Skip special tokens\n",
    "        if start_char == 0 and end_char == 0:\n",
    "            continue\n",
    "        \n",
    "        pred_label = id2label[pred_id]\n",
    "        \n",
    "        # Handle BIO tags\n",
    "        if pred_label.startswith('B-'):\n",
    "            # Save previous entity if exists\n",
    "            if current_entity:\n",
    "                entities.append(current_entity)\n",
    "            \n",
    "            # Start new entity\n",
    "            label = pred_label[2:]  # Remove 'B-' prefix\n",
    "            current_entity = {\n",
    "                'label': label,\n",
    "                'start': start_char,\n",
    "                'end': end_char,\n",
    "                'text': text[start_char:end_char]\n",
    "            }\n",
    "        \n",
    "        elif pred_label.startswith('I-') and current_entity:\n",
    "            # Continue current entity\n",
    "            label = pred_label[2:]  # Remove 'I-' prefix\n",
    "            if label == current_entity['label']:\n",
    "                current_entity['end'] = end_char\n",
    "                current_entity['text'] = text[current_entity['start']:end_char]\n",
    "        \n",
    "        else:  # 'O' or mismatched I- tag\n",
    "            if current_entity:\n",
    "                entities.append(current_entity)\n",
    "                current_entity = None\n",
    "    \n",
    "    # Don't forget the last entity\n",
    "    if current_entity:\n",
    "        entities.append(current_entity)\n",
    "    \n",
    "    return entities\n",
    "\n",
    "print(\"Prediction function defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Generate Predictions on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate predictions for all test documents\n",
    "print(\"Generating predictions on test set...\")\n",
    "model.eval()\n",
    "\n",
    "all_predictions = []\n",
    "\n",
    "for i, doc in enumerate(test_data):\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Processing document {i}/{len(test_data)}\")\n",
    "    \n",
    "    document_id = doc['document_id']\n",
    "    text = doc['full_text']\n",
    "    \n",
    "    # Get predictions\n",
    "    entities = predict_entities(text, model, tokenizer)\n",
    "    \n",
    "    # Store predictions with document_id\n",
    "    for entity in entities:\n",
    "        all_predictions.append({\n",
    "            'document_id': document_id,\n",
    "            'label': entity['label'],\n",
    "            'start_offset': entity['start'],\n",
    "            'end_offset': entity['end']\n",
    "        })\n",
    "\n",
    "print(f\"\\nTotal predictions made: {len(all_predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Create Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 18. Create Submission File (Match row_id exactly with sample_submission)\n",
    "# =============================================================================\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load sample submission\n",
    "sample_sub = pd.read_csv('/root/sample_submission.csv')\n",
    "\n",
    "print(f\"Sample submission rows: {len(sample_sub)}\")\n",
    "print(f\"Total predictions made: {len(all_predictions)}\")\n",
    "\n",
    "# Group predictions by document_id for O(1) lookup\n",
    "predictions_by_doc = defaultdict(list)\n",
    "for pred in all_predictions:\n",
    "    predictions_by_doc[pred['document_id']].append({\n",
    "        'start_offset': pred['start_offset'],\n",
    "        'end_offset': pred['end_offset'],\n",
    "        'label': pred['label']\n",
    "    })\n",
    "\n",
    "# Sort predictions within each document by start_offset\n",
    "for doc_id in predictions_by_doc:\n",
    "    predictions_by_doc[doc_id].sort(key=lambda x: x['start_offset'])\n",
    "\n",
    "print(f\"Documents with predictions: {len(predictions_by_doc)}\")\n",
    "\n",
    "# Track current index for each document\n",
    "doc_pred_index = defaultdict(int)\n",
    "\n",
    "# Create submission matching sample_submission structure\n",
    "submission_data = []\n",
    "filled_count = 0\n",
    "\n",
    "for _, row in sample_sub.iterrows():\n",
    "    doc_id = row['document_id']\n",
    "    idx = doc_pred_index[doc_id]\n",
    "    \n",
    "    if doc_id in predictions_by_doc and idx < len(predictions_by_doc[doc_id]):\n",
    "        # We have a prediction for this slot\n",
    "        pred = predictions_by_doc[doc_id][idx]\n",
    "        submission_data.append({\n",
    "            'row_id': row['row_id'],\n",
    "            'document_id': doc_id,\n",
    "            'start_offset': pred['start_offset'],\n",
    "            'end_offset': pred['end_offset'],\n",
    "            'label': pred['label']\n",
    "        })\n",
    "        doc_pred_index[doc_id] += 1\n",
    "        filled_count += 1\n",
    "    else:\n",
    "        # No prediction - use placeholder (dummy prediction)\n",
    "        submission_data.append({\n",
    "            'row_id': row['row_id'],\n",
    "            'document_id': doc_id,\n",
    "            'start_offset': 0,\n",
    "            'end_offset': 1,\n",
    "            'label': 'NAME'  \n",
    "        })\n",
    "\n",
    "submission_df = pd.DataFrame(submission_data)\n",
    "\n",
    "# Save submission\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(f\"\\nSubmission file created: submission.csv\")\n",
    "print(f\"Submission shape: {submission_df.shape}\")\n",
    "print(f\"Predictions filled: {filled_count}\")\n",
    "print(f\"Placeholder rows: {len(submission_df) - filled_count}\")\n",
    "\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(submission_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19. Save Model (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save the trained model and tokenizer\n",
    "model.save_pretrained(\"./bert-pii-model\")\n",
    "tokenizer.save_pretrained(\"./bert-pii-model\")\n",
    "\n",
    "print(\"Model and tokenizer saved to './bert-pii-model'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
