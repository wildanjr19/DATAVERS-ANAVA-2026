{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20cf604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import joblib \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler # Ditambahkan StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from sklearn.experimental import enable_iterative_imputer \n",
    "from sklearn.impute import IterativeImputer \n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Feature Selection Imports (DITAMBAHKAN)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Model\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Untuk jarak geografis\n",
    "from math import radians, sin, cos, sqrt, asin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ac013a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. LOAD DATA\n",
    "print(\"Loading data...\")\n",
    "train_df = pd.read_csv('/kaggle/input/data-anv-v1/train.csv')\n",
    "test_df = pd.read_csv('/kaggle/input/data-anv-v1/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d477d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_data_types(df):\n",
    "    df['Timestamp'] = pd.to_datetime(df['Timestamp'], errors='coerce')\n",
    "    \n",
    "    if df['Battery_Level'].dtype == 'object':\n",
    "        df['Battery_Level'] = df['Battery_Level'].astype(str).str.replace('%', '', regex=False)\n",
    "        df['Battery_Level'] = pd.to_numeric(df['Battery_Level'], errors='coerce')\n",
    "    \n",
    "    float_cols = df.select_dtypes(include=['float64']).columns\n",
    "    for col in float_cols:\n",
    "        df[col] = df[col].astype('float32')\n",
    "            \n",
    "    df['Device_FP'] = df['Device_FP'].astype(str).replace('nan', np.nan)\n",
    "    return df\n",
    "\n",
    "train_df = fix_data_types(train_df)\n",
    "test_df = fix_data_types(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf342882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. SAMPLING\n",
    "def stratified_sampling(df, target_col, sample_frac=0.20, random_state=42):\n",
    "    df_sampled = df.groupby(target_col, group_keys=False).apply(\n",
    "        lambda x: x.sample(frac=sample_frac, random_state=random_state)\n",
    "    )\n",
    "    return df_sampled.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4dc88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling train data\n",
    "train_sampled = stratified_sampling(train_df, 'Trip_Label', sample_frac=0.20)\n",
    "train_labels = train_sampled['Trip_Label'].copy()\n",
    "train_sampled = train_sampled.drop('Trip_Label', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96e2e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tandai mana train mana test\n",
    "train_sampled['is_train'] = 1\n",
    "test_df['is_train'] = 0\n",
    "\n",
    "# Gabungkan\n",
    "df_all = pd.concat([train_sampled, test_df], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4debf0ef",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70a05e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. FEATURE ENGINEERING\n",
    "print(\"Generating features...\")\n",
    "\n",
    "# --- A. Fitur Temporal ---\n",
    "df_all['Timestamp'] = pd.to_datetime(df_all['Timestamp'])\n",
    "df_all['hour'] = df_all['Timestamp'].dt.hour\n",
    "df_all['day_of_week'] = df_all['Timestamp'].dt.dayofweek\n",
    "df_all['is_weekend'] = (df_all['day_of_week'] >= 5).astype(int)\n",
    "df_all['is_rush_hour'] = ((df_all['hour'] >= 7) & (df_all['hour'] <= 9) | \n",
    "                           (df_all['hour'] >= 17) & (df_all['hour'] <= 19)).astype(int)\n",
    "df_all['is_late_night'] = ((df_all['hour'] >= 23) | (df_all['hour'] <= 5)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40968af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- B. Fitur Spasial (Haversine) ---\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    # Vectorized haversine\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    return 6371 * c\n",
    "\n",
    "df_all['haversine_dist'] = haversine_distance(\n",
    "    df_all['Pickup_Lat'], df_all['Pickup_Long'],\n",
    "    df_all['Dropoff_Lat'], df_all['Dropoff_Long']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c6b464",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- C. Penanganan Sensor Hilang (SOGP Logic) ---\n",
    "sensor_cols = ['Accel_X', 'Accel_Y', 'Accel_Z', 'Gyro_X', 'Gyro_Y', 'Gyro_Z']\n",
    "available_sensors = [c for c in sensor_cols if c in df_all.columns]\n",
    "\n",
    "if available_sensors:\n",
    "    df_all['missing_sensor_count'] = df_all[available_sensors].isnull().sum(axis=1)\n",
    "    df_all['is_sensor_data_missing'] = (df_all['missing_sensor_count'] > 0).astype(int)\n",
    "    \n",
    "    mice_imputer = IterativeImputer(max_iter=5, random_state=42)\n",
    "    df_all[available_sensors] = mice_imputer.fit_transform(df_all[available_sensors])\n",
    "else:\n",
    "    df_all['missing_sensor_count'] = 0\n",
    "    df_all['is_sensor_data_missing'] = 0\n",
    "\n",
    "# --- D. Telematics Standard ---\n",
    "df_all['accel_magnitude'] = np.sqrt(df_all['Accel_X']**2 + df_all['Accel_Y']**2 + df_all['Accel_Z']**2)\n",
    "df_all['vertical_jerk'] = np.abs(df_all['Accel_Z'] - 9.8)\n",
    "df_all['harsh_accel_count'] = (\n",
    "    (np.abs(df_all['Accel_X']) > df_all['Accel_X'].quantile(0.95)).astype(int) + \n",
    "    (np.abs(df_all['Accel_Y']) > df_all['Accel_Y'].quantile(0.95)).astype(int) + \n",
    "    (np.abs(df_all['Accel_Z'] - 9.8) > 2.0).astype(int)\n",
    ")\n",
    "\n",
    "# --- E. Separator Logic (Nav vs Service) ---\n",
    "df_all['is_battery_saver'] = (df_all['Battery_Level'] <= 20).astype(int)\n",
    "df_all['is_critical_battery'] = (df_all['Battery_Level'] <= 10).astype(int)\n",
    "\n",
    "signal_map = {'No Signal': 0.1, '2G': 1, 'Edge': 2, '3G': 3, '4G': 4, '5G': 5}\n",
    "if 'Signal_Strength' in df_all.columns:\n",
    "    df_all['signal_score'] = df_all['Signal_Strength'].map(signal_map).fillna(3)\n",
    "else:\n",
    "    df_all['signal_score'] = 3\n",
    "\n",
    "df_all['gps_reliability'] = (1 / (df_all['GPS_Accuracy_M'] + 1)) * df_all['signal_score'] * (1 - (df_all['is_battery_saver'] * 0.8))\n",
    "\n",
    "df_all['dist_deviation'] = np.abs(df_all['haversine_dist'] - df_all['Distance_KM'])\n",
    "df_all['dist_deviation_ratio'] = df_all['dist_deviation'] / (df_all['haversine_dist'] + 0.01)\n",
    "\n",
    "df_all['prob_nav_issue'] = df_all['dist_deviation_ratio'] / (df_all['gps_reliability'] + 0.001)\n",
    "df_all['prob_service_fraud'] = df_all['dist_deviation_ratio'] * df_all['gps_reliability']\n",
    "\n",
    "# --- F. Fitur Lainnya ---\n",
    "df_all['has_promo'] = (~df_all['Promo_Code'].isna()).astype(int)\n",
    "df_all['price_per_km'] = df_all['Est_Price_IDR'] / (df_all['Distance_KM'] + 1e-5)\n",
    "\n",
    "df_all['Device_FP'] = df_all['Device_FP'].fillna('Unknown-Unknown-0-v0.0')\n",
    "device_split = df_all['Device_FP'].str.split('-', expand=True)\n",
    "df_all['temp_brand'] = device_split[0]\n",
    "df_all['temp_model'] = device_split[1]\n",
    "valid_device_map = {\"Apple\": [\"iPhone\"], \"Samsung\": [\"Galaxy\"], \"Oppo\": [\"Reno\"], \"Vivo\": [\"Y_Series\"], \"Xiaomi\": [\"Redmi\"], \"Infinix\": [\"Hot\"]}\n",
    "\n",
    "def check_device_mismatch(row):\n",
    "    if row['temp_brand'] in valid_device_map:\n",
    "        if row['temp_model'] in valid_device_map[row['temp_brand']]:\n",
    "            return 0 \n",
    "    return 1\n",
    "df_all['device_mismatch'] = df_all.apply(check_device_mismatch, axis=1)\n",
    "df_all.drop(columns=['temp_brand', 'temp_model'], inplace=True)\n",
    "\n",
    "# 4. CLEANUP & ENCODING\n",
    "cols_to_drop = [\n",
    "    'Timestamp', 'Pickup_Lat', 'Pickup_Long', 'Dropoff_Lat', 'Dropoff_Long', \n",
    "    'Promo_Code', 'lat_diff', 'long_diff', 'Pickup_Zone', 'Dropoff_Zone', 'Car_Model'\n",
    "]\n",
    "df_all = df_all.drop(columns=[c for c in cols_to_drop if c in df_all.columns])\n",
    "\n",
    "categorical_features = ['Payment_Method', 'Device_FP', 'Signal_Strength', 'Weather', 'Traffic']\n",
    "label_encoders = {}\n",
    "for col in categorical_features:\n",
    "    if col in df_all.columns:\n",
    "        le = LabelEncoder()\n",
    "        df_all[col] = df_all[col].astype(str)\n",
    "        le.fit(df_all[col])\n",
    "        df_all[col] = le.transform(df_all[col])\n",
    "        label_encoders[col] = le\n",
    "\n",
    "obj_cols = df_all.select_dtypes(include=['object']).columns\n",
    "cols_to_remove = [c for c in obj_cols if c != 'Trip_ID']\n",
    "if cols_to_remove:\n",
    "    df_all = df_all.drop(columns=cols_to_remove)\n",
    "\n",
    "# 5. SPLIT DATA\n",
    "train_final = df_all[df_all['is_train'] == 1].copy()\n",
    "test_final = df_all[df_all['is_train'] == 0].copy()\n",
    "\n",
    "train_final = train_final.drop(['is_train', 'Trip_ID'], axis=1)\n",
    "test_ids = test_final['Trip_ID'].values \n",
    "test_final = test_final.drop(['is_train', 'Trip_ID'], axis=1)\n",
    "\n",
    "le_target = LabelEncoder()\n",
    "y_encoded = le_target.fit_transform(train_labels)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_final, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "# ==============================================================================\n",
    "# FEATURE SELECTION: LOGISTIC REGRESSION (LASSO/L1) - MULTINOMIAL\n",
    "# ==============================================================================\n",
    "print(\"\\n=== STARTING FEATURE SELECTION (L1 LASSO) ===\")\n",
    "print(f\"Features before selection: {X_train.shape[1]}\")\n",
    "\n",
    "# 1. Scaling (Wajib untuk Logistic Regression agar penaltinya adil)\n",
    "scaler = StandardScaler()\n",
    "# Kita impute 0 hanya untuk proses seleksi fitur agar LR tidak error kena NaN (LGBM nanti tetap pakai NaN asli)\n",
    "X_train_clean_for_select = X_train.fillna(0) \n",
    "X_train_scaled = scaler.fit_transform(X_train_clean_for_select)\n",
    "\n",
    "# 2. Definisikan Logistic Regression dengan L1 (Lasso)\n",
    "# Solver 'saga' diperlukan untuk L1 pada Multiclass\n",
    "lasso_selector = LogisticRegression(\n",
    "    penalty='l1',            # L1 Regularization (Lasso)\n",
    "    solver='saga',           # Solver yang support L1\n",
    "    multi_class='multinomial', \n",
    "    C=0.5,                   # Inverse strength (Makin kecil C, makin sedikit fitur terpilih)\n",
    "    random_state=42,\n",
    "    max_iter=1000,           # Iterasi tinggi agar konvergen\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# 3. SelectFromModel\n",
    "selector = SelectFromModel(estimator=lasso_selector)\n",
    "selector.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 4. Ambil Mask Fitur Terpilih\n",
    "selected_mask = selector.get_support()\n",
    "selected_columns = X_train.columns[selected_mask]\n",
    "\n",
    "print(f\"Features selected: {len(selected_columns)}\")\n",
    "print(f\"Dropped features: {len(X_train.columns) - len(selected_columns)}\")\n",
    "print(f\"List Selected: {list(selected_columns)}\")\n",
    "\n",
    "# 5. Terapkan Seleksi ke DataFrame Asli (X_train, X_val, test_final)\n",
    "X_train = X_train[selected_columns]\n",
    "X_val = X_val[selected_columns]\n",
    "test_final = test_final[selected_columns]\n",
    "\n",
    "print(\"Feature selection applied successfully.\\n\")\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "# Class Weights\n",
    "classes = np.unique(y_train)\n",
    "weights = compute_class_weight('balanced', classes=classes, y=y_train)\n",
    "class_weight_dict = dict(zip(classes, weights))\n",
    "print(f\"Class Weights: {class_weight_dict}\")\n",
    "\n",
    "# 6. MODELING (LGBM Optimized)\n",
    "lgb_params = {\n",
    "    'objective': 'multiclass',\n",
    "    'n_estimators': 6200,\n",
    "    'learning_rate': 0.015,\n",
    "    'num_leaves': 90,\n",
    "    'max_depth': 12,\n",
    "    'min_child_samples': 40,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'reg_alpha': 0.5,\n",
    "    'reg_lambda': 0.5,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "    'class_weight': class_weight_dict,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "lgb_model = lgb.LGBMClassifier(**lgb_params)\n",
    "\n",
    "print(\"Training Initial LightGBM (with Selected Features)...\")\n",
    "lgb_model.fit(\n",
    "    X_train, \n",
    "    y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    eval_metric='multi_logloss',\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=100),\n",
    "        lgb.log_evaluation(period=200)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 7. EVALUASI AWAL\n",
    "y_val_pred = lgb_model.predict(X_val)\n",
    "macro_f1 = f1_score(y_val, y_val_pred, average='macro')\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"INITIAL MACRO F1-SCORE (VALIDATION): {macro_f1:.4f}\")\n",
    "print(f\"{'='*50}\\n\")\n",
    "print(classification_report(y_val, y_val_pred, target_names=le_target.classes_))\n",
    "\n",
    "# 8. PSEUDO LABELING\n",
    "print(\"\\n=== STARTING PSEUDO LABELING ===\")\n",
    "\n",
    "# 1. Prediksi probabilitas pada data TEST (yang sudah dikurangi fiturnya)\n",
    "print(\"Predicting test data probabilities...\")\n",
    "test_probs = lgb_model.predict_proba(test_final)\n",
    "\n",
    "# 2. Ambil prediksi label\n",
    "test_preds_idx = np.argmax(test_probs, axis=1)\n",
    "test_confidence = np.max(test_probs, axis=1)\n",
    "\n",
    "# 3. Filter data dengan confidence > 95%\n",
    "high_conf_mask = test_confidence > 0.95\n",
    "X_pseudo = test_final[high_conf_mask].copy()\n",
    "y_pseudo = test_preds_idx[high_conf_mask]\n",
    "\n",
    "print(f\"Total data test: {len(test_final)}\")\n",
    "print(f\"Data dengan confidence > 95%: {len(X_pseudo)} ({len(X_pseudo)/len(test_final)*100:.2f}%)\")\n",
    "\n",
    "if len(X_pseudo) > 0:\n",
    "    # 4. Gabungkan Data Train Awal + Data Pseudo\n",
    "    print(\"Combining Train Data with Pseudo Data...\")\n",
    "    X_train_augmented = pd.concat([X_train, X_pseudo], axis=0)\n",
    "    y_train_augmented = np.concatenate([y_train, y_pseudo], axis=0)\n",
    "    \n",
    "    # 5. Retrain Model\n",
    "    print(\"Retraining LightGBM with Pseudo Labels...\")\n",
    "    \n",
    "    classes_aug = np.unique(y_train_augmented)\n",
    "    weights_aug = compute_class_weight('balanced', classes=classes_aug, y=y_train_augmented)\n",
    "    class_weight_dict_aug = dict(zip(classes_aug, weights_aug))\n",
    "    \n",
    "    lgb_params['class_weight'] = class_weight_dict_aug\n",
    "    \n",
    "    lgb_model_pseudo = lgb.LGBMClassifier(**lgb_params)\n",
    "    \n",
    "    lgb_model_pseudo.fit(\n",
    "        X_train_augmented, \n",
    "        y_train_augmented,\n",
    "        eval_set=[(X_val, y_val)], \n",
    "        eval_metric='multi_logloss',\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=100),\n",
    "            lgb.log_evaluation(period=200)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    y_val_pred_pseudo = lgb_model_pseudo.predict(X_val)\n",
    "    macro_f1_pseudo = f1_score(y_val, y_val_pred_pseudo, average='macro')\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"PSEUDO LABELING MACRO F1-SCORE (VALIDATION): {macro_f1_pseudo:.4f}\")\n",
    "    print(f\"Improvement: {macro_f1_pseudo - macro_f1:.4f}\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "    \n",
    "    final_model = lgb_model_pseudo\n",
    "    final_model_name = 'lgbm_model_pseudo_lasso.pkl'\n",
    "else:\n",
    "    print(\"Tidak ada data yang memenuhi threshold confidence > 95%. Menggunakan model awal.\")\n",
    "    final_model = lgb_model\n",
    "    final_model_name = 'lgbm_model_initial_lasso.pkl'\n",
    "\n",
    "# 9. SUBMISSION FINAL\n",
    "print(\"Creating final submission...\")\n",
    "y_test_pred_idx_final = final_model.predict(test_final)\n",
    "y_test_pred_label_final = le_target.inverse_transform(y_test_pred_idx_final)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'Trip_ID': test_ids,\n",
    "    'Trip_Label': y_test_pred_label_final\n",
    "})\n",
    "\n",
    "submission.to_csv('submission_lgbm_lasso_pseudo.csv', index=False)\n",
    "print(\"Selesai! File: submission_lgbm_lasso_pseudo.csv\")\n",
    "\n",
    "# 10. SAVE MODEL\n",
    "print(\"Saving final model to .pkl...\")\n",
    "joblib.dump(final_model, final_model_name)\n",
    "print(f\"Model berhasil disimpan: {final_model_name}\")\n",
    "\n",
    "# Tambahan: Save juga dengan format .joblib untuk efisiensi\n",
    "final_model_name_joblib = final_model_name.replace('.pkl', '.joblib')\n",
    "joblib.dump(final_model, final_model_name_joblib, compress=3)\n",
    "print(f\"Model juga disimpan dalam format .joblib: {final_model_name_joblib}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b5effc",
   "metadata": {},
   "source": [
    "# üéØ Feature Importance & SHAP Analysis\n",
    "\n",
    "Bagian ini menganalisis fitur-fitur yang paling berpengaruh dalam model menggunakan Feature Importance dari LightGBM dan SHAP values untuk interpretability yang lebih mendalam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e48cbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance dari LightGBM\n",
    "print(\"=\"*60)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS - LGBM\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Ambil feature importance dari final model\n",
    "feature_importance = final_model.feature_importances_\n",
    "feature_names = selected_columns.tolist()\n",
    "\n",
    "# Buat DataFrame\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': feature_importance\n",
    "})\n",
    "\n",
    "# Sort by importance\n",
    "importance_df = importance_df.sort_values('Importance', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Hitung persentase\n",
    "importance_df['Importance_Pct'] = (importance_df['Importance'] / importance_df['Importance'].sum() * 100).round(2)\n",
    "importance_df['Cumulative_Pct'] = importance_df['Importance_Pct'].cumsum().round(2)\n",
    "\n",
    "# Tampilkan Top 20 fitur\n",
    "print(\"\\nüîù Top 20 Fitur Paling Penting:\")\n",
    "print(importance_df.head(20).to_string(index=False))\n",
    "\n",
    "# Simpan ke CSV\n",
    "importance_df.to_csv('feature_importance_lgbm.csv', index=False)\n",
    "print(f\"\\n‚úÖ Feature importance disimpan: feature_importance_lgbm.csv\")\n",
    "\n",
    "# Visualisasi Top 20 Features\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Plot 1: Bar plot horizontal untuk Top 20\n",
    "top_20 = importance_df.head(20)\n",
    "axes[0].barh(range(len(top_20)), top_20['Importance'], color='steelblue')\n",
    "axes[0].set_yticks(range(len(top_20)))\n",
    "axes[0].set_yticklabels(top_20['Feature'])\n",
    "axes[0].invert_yaxis()\n",
    "axes[0].set_xlabel('Importance Score', fontsize=10)\n",
    "axes[0].set_title('Top 20 Feature Importance (LightGBM)', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Plot 2: Cumulative importance\n",
    "axes[1].plot(range(1, len(importance_df)+1), importance_df['Cumulative_Pct'], \n",
    "             color='darkgreen', linewidth=2)\n",
    "axes[1].axhline(y=80, color='red', linestyle='--', linewidth=1, label='80% Threshold')\n",
    "axes[1].axhline(y=90, color='orange', linestyle='--', linewidth=1, label='90% Threshold')\n",
    "axes[1].set_xlabel('Number of Features', fontsize=10)\n",
    "axes[1].set_ylabel('Cumulative Importance (%)', fontsize=10)\n",
    "axes[1].set_title('Cumulative Feature Importance', fontsize=12, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance_lgbm.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Plot feature importance disimpan: feature_importance_lgbm.png\")\n",
    "\n",
    "# Analisis threshold\n",
    "n_features_80 = (importance_df['Cumulative_Pct'] <= 80).sum()\n",
    "n_features_90 = (importance_df['Cumulative_Pct'] <= 90).sum()\n",
    "print(f\"\\nüìä Analisis:\")\n",
    "print(f\"   - Fitur yang berkontribusi 80% importance: {n_features_80}/{len(importance_df)}\")\n",
    "print(f\"   - Fitur yang berkontribusi 90% importance: {n_features_90}/{len(importance_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2de400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install SHAP jika belum ada (untuk Kaggle environment)\n",
    "try:\n",
    "    import shap\n",
    "    print(\"‚úÖ SHAP library sudah terinstall\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è SHAP library belum terinstall. Menginstall...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"shap\"])\n",
    "    import shap\n",
    "    print(\"‚úÖ SHAP library berhasil diinstall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ea6a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Analysis - Summary Plot\n",
    "print(\"=\"*60)\n",
    "print(\"SHAP VALUES ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Gunakan subset data untuk SHAP (untuk efisiensi komputasi)\n",
    "# Karena SHAP computation mahal, ambil sample\n",
    "sample_size = min(1000, len(X_val))\n",
    "X_val_sample = X_val.sample(n=sample_size, random_state=42)\n",
    "\n",
    "print(f\"\\nüîÑ Menghitung SHAP values untuk {sample_size} sampel validasi...\")\n",
    "print(\"   (Proses ini mungkin memakan waktu beberapa menit)\")\n",
    "\n",
    "# Create SHAP explainer\n",
    "explainer = shap.TreeExplainer(final_model)\n",
    "\n",
    "# Calculate SHAP values\n",
    "shap_values = explainer.shap_values(X_val_sample)\n",
    "\n",
    "print(\"‚úÖ SHAP values berhasil dihitung\")\n",
    "\n",
    "# Info tentang SHAP values\n",
    "if isinstance(shap_values, list):\n",
    "    print(f\"   - SHAP values shape: {len(shap_values)} classes √ó {shap_values[0].shape}\")\n",
    "else:\n",
    "    print(f\"   - SHAP values shape: {shap_values.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcebac57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Summary Plot - Global Feature Importance\n",
    "print(\"=\"*60)\n",
    "print(\"SHAP SUMMARY PLOT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Summary plot (untuk multiclass, SHAP akan aggregate semua classes)\n",
    "shap.summary_plot(shap_values, X_val_sample, plot_type=\"bar\", show=False)\n",
    "plt.title('SHAP Feature Importance (Mean |SHAP Value|)', fontsize=12, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig('shap_summary_bar.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ SHAP summary bar plot disimpan: shap_summary_bar.png\")\n",
    "\n",
    "# Summary plot dengan distribusi\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "shap.summary_plot(shap_values, X_val_sample, show=False)\n",
    "plt.title('SHAP Feature Impact Distribution', fontsize=12, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig('shap_summary_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ SHAP distribution plot disimpan: shap_summary_distribution.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6df7968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Values per Class\n",
    "print(\"=\"*60)\n",
    "print(\"SHAP ANALYSIS PER CLASS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Untuk multiclass, SHAP values berbentuk list (satu untuk tiap class)\n",
    "if isinstance(shap_values, list):\n",
    "    n_classes = len(shap_values)\n",
    "    class_names = le_target.classes_\n",
    "    \n",
    "    print(f\"\\nüìä Total Classes: {n_classes}\")\n",
    "    print(f\"   Classes: {', '.join(class_names)}\\n\")\n",
    "    \n",
    "    # Buat plot untuk setiap class\n",
    "    fig, axes = plt.subplots(n_classes, 1, figsize=(12, 6*n_classes))\n",
    "    \n",
    "    if n_classes == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, (class_shap, class_name) in enumerate(zip(shap_values, class_names)):\n",
    "        print(f\"üîç Class: {class_name}\")\n",
    "        \n",
    "        # Hitung mean absolute SHAP values untuk class ini\n",
    "        mean_abs_shap = np.abs(class_shap).mean(axis=0)\n",
    "        \n",
    "        # Buat DataFrame\n",
    "        shap_class_df = pd.DataFrame({\n",
    "            'Feature': selected_columns,\n",
    "            'Mean_Abs_SHAP': mean_abs_shap\n",
    "        }).sort_values('Mean_Abs_SHAP', ascending=False)\n",
    "        \n",
    "        print(f\"   Top 5 Features: {', '.join(shap_class_df.head(5)['Feature'].tolist())}\\n\")\n",
    "        \n",
    "        # Plot Top 15 untuk class ini\n",
    "        top_15 = shap_class_df.head(15)\n",
    "        axes[idx].barh(range(len(top_15)), top_15['Mean_Abs_SHAP'], color=f'C{idx}')\n",
    "        axes[idx].set_yticks(range(len(top_15)))\n",
    "        axes[idx].set_yticklabels(top_15['Feature'])\n",
    "        axes[idx].invert_yaxis()\n",
    "        axes[idx].set_xlabel('Mean |SHAP Value|', fontsize=10)\n",
    "        axes[idx].set_title(f'Top 15 Features for Class: {class_name}', \n",
    "                           fontsize=11, fontweight='bold')\n",
    "        axes[idx].grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        # Simpan per class\n",
    "        shap_class_df.to_csv(f'shap_values_class_{class_name}.csv', index=False)\n",
    "        print(f\"   ‚úÖ Disimpan: shap_values_class_{class_name}.csv\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('shap_per_class.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n‚úÖ SHAP per class plot disimpan: shap_per_class.png\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è SHAP values tidak dalam format multiclass list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb73ed8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Dependence Plot untuk Top Features\n",
    "print(\"=\"*60)\n",
    "print(\"SHAP DEPENDENCE PLOTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Ambil top 6 features dari feature importance\n",
    "top_features_for_dependence = importance_df.head(6)['Feature'].tolist()\n",
    "\n",
    "print(f\"üìä Membuat dependence plots untuk top {len(top_features_for_dependence)} fitur:\\n\")\n",
    "\n",
    "# Untuk multiclass, ambil SHAP values dari class pertama (atau bisa dipilih)\n",
    "if isinstance(shap_values, list):\n",
    "    shap_for_dependence = shap_values[0]  # Class pertama\n",
    "    selected_class = le_target.classes_[0]\n",
    "    print(f\"   Menggunakan SHAP values untuk class: {selected_class}\\n\")\n",
    "else:\n",
    "    shap_for_dependence = shap_values\n",
    "\n",
    "# Buat grid plot\n",
    "n_cols = 2\n",
    "n_rows = (len(top_features_for_dependence) + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, 5*n_rows))\n",
    "axes = axes.flatten() if n_rows > 1 else axes\n",
    "\n",
    "for idx, feature in enumerate(top_features_for_dependence):\n",
    "    print(f\"   - {feature}\")\n",
    "    \n",
    "    # Get feature index\n",
    "    feature_idx = list(selected_columns).index(feature)\n",
    "    \n",
    "    # Plot pada subplot\n",
    "    plt.sca(axes[idx])\n",
    "    shap.dependence_plot(\n",
    "        feature_idx, \n",
    "        shap_for_dependence, \n",
    "        X_val_sample,\n",
    "        show=False,\n",
    "        ax=axes[idx]\n",
    "    )\n",
    "    axes[idx].set_title(f'SHAP Dependence: {feature}', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Hide unused subplots\n",
    "for idx in range(len(top_features_for_dependence), len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('shap_dependence_plots.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ SHAP dependence plots disimpan: shap_dependence_plots.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3e4082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Comparison: Feature Importance vs SHAP\n",
    "print(\"=\"*60)\n",
    "print(\"COMPARISON: LGBM FEATURE IMPORTANCE vs SHAP\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Hitung mean absolute SHAP values (aggregate across all classes)\n",
    "if isinstance(shap_values, list):\n",
    "    # Average across all classes\n",
    "    mean_abs_shap_all = np.mean([np.abs(sv).mean(axis=0) for sv in shap_values], axis=0)\n",
    "else:\n",
    "    mean_abs_shap_all = np.abs(shap_values).mean(axis=0)\n",
    "\n",
    "# Buat DataFrame comparison\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Feature': selected_columns,\n",
    "    'LGBM_Importance': feature_importance,\n",
    "    'SHAP_MeanAbs': mean_abs_shap_all\n",
    "})\n",
    "\n",
    "# Normalize untuk perbandingan\n",
    "comparison_df['LGBM_Norm'] = (comparison_df['LGBM_Importance'] / comparison_df['LGBM_Importance'].max() * 100).round(2)\n",
    "comparison_df['SHAP_Norm'] = (comparison_df['SHAP_MeanAbs'] / comparison_df['SHAP_MeanAbs'].max() * 100).round(2)\n",
    "\n",
    "# Hitung korelasi ranking\n",
    "comparison_df['LGBM_Rank'] = comparison_df['LGBM_Importance'].rank(ascending=False)\n",
    "comparison_df['SHAP_Rank'] = comparison_df['SHAP_MeanAbs'].rank(ascending=False)\n",
    "comparison_df['Rank_Diff'] = np.abs(comparison_df['LGBM_Rank'] - comparison_df['SHAP_Rank'])\n",
    "\n",
    "# Sort by LGBM importance\n",
    "comparison_df = comparison_df.sort_values('LGBM_Importance', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"\\nüîù Top 15 Features Comparison:\")\n",
    "print(comparison_df[['Feature', 'LGBM_Norm', 'SHAP_Norm', 'LGBM_Rank', 'SHAP_Rank', 'Rank_Diff']].head(15).to_string(index=False))\n",
    "\n",
    "# Simpan comparison\n",
    "comparison_df.to_csv('feature_importance_comparison.csv', index=False)\n",
    "print(f\"\\n‚úÖ Comparison disimpan: feature_importance_comparison.csv\")\n",
    "\n",
    "# Visualisasi comparison untuk Top 15\n",
    "top_15_comp = comparison_df.head(15)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Normalized comparison\n",
    "x_pos = np.arange(len(top_15_comp))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].barh(x_pos - width/2, top_15_comp['LGBM_Norm'], width, \n",
    "            label='LGBM Importance', color='steelblue', alpha=0.8)\n",
    "axes[0].barh(x_pos + width/2, top_15_comp['SHAP_Norm'], width, \n",
    "            label='SHAP Mean |Value|', color='coral', alpha=0.8)\n",
    "axes[0].set_yticks(x_pos)\n",
    "axes[0].set_yticklabels(top_15_comp['Feature'])\n",
    "axes[0].invert_yaxis()\n",
    "axes[0].set_xlabel('Normalized Importance (0-100)', fontsize=10)\n",
    "axes[0].set_title('Top 15: LGBM vs SHAP Importance', fontsize=12, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Plot 2: Rank difference\n",
    "axes[1].barh(range(len(top_15_comp)), top_15_comp['Rank_Diff'], color='green', alpha=0.6)\n",
    "axes[1].set_yticks(range(len(top_15_comp)))\n",
    "axes[1].set_yticklabels(top_15_comp['Feature'])\n",
    "axes[1].invert_yaxis()\n",
    "axes[1].set_xlabel('Rank Difference (|LGBM Rank - SHAP Rank|)', fontsize=10)\n",
    "axes[1].set_title('Ranking Consistency (Lower = More Consistent)', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('lgbm_vs_shap_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Comparison plot disimpan: lgbm_vs_shap_comparison.png\")\n",
    "\n",
    "# Correlation analysis\n",
    "from scipy.stats import spearmanr\n",
    "corr, p_value = spearmanr(comparison_df['LGBM_Rank'], comparison_df['SHAP_Rank'])\n",
    "print(f\"\\nüìà Spearman Rank Correlation: {corr:.4f} (p-value: {p_value:.4e})\")\n",
    "print(f\"   Interpretasi: {'Sangat Konsisten' if corr > 0.8 else 'Cukup Konsisten' if corr > 0.6 else 'Kurang Konsisten'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1216f8",
   "metadata": {},
   "source": [
    "# üìä Analisis Data Setelah Sampling\n",
    "\n",
    "Bagian ini menganalisis data training setelah proses stratified sampling untuk memahami distribusi dan karakteristik data yang akan digunakan untuk training model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46aa6844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hitung jumlah data dan proporsi kelas setelah sampling\n",
    "print(\"=\"*60)\n",
    "print(\"INFORMASI DATA SETELAH SAMPLING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüìå Total Data Awal (Train): {len(train_df):,}\")\n",
    "print(f\"üìå Total Data Setelah Sampling: {len(train_sampled):,}\")\n",
    "print(f\"üìå Persentase Sampling: {len(train_sampled)/len(train_df)*100:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DISTRIBUSI KELAS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Hitung distribusi kelas\n",
    "class_distribution = train_labels.value_counts().sort_index()\n",
    "class_percentage = (class_distribution / len(train_labels) * 100).round(2)\n",
    "\n",
    "# Buat DataFrame untuk visualisasi yang lebih baik\n",
    "dist_df = pd.DataFrame({\n",
    "    'Kelas': class_distribution.index,\n",
    "    'Jumlah': class_distribution.values,\n",
    "    'Persentase (%)': class_percentage.values\n",
    "})\n",
    "\n",
    "print(dist_df.to_string(index=False))\n",
    "\n",
    "# Visualisasi distribusi kelas\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar plot\n",
    "dist_df.plot(x='Kelas', y='Jumlah', kind='bar', ax=axes[0], color='steelblue', legend=False)\n",
    "axes[0].set_title('Distribusi Jumlah Sampel per Kelas', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Kelas', fontsize=10)\n",
    "axes[0].set_ylabel('Jumlah Sampel', fontsize=10)\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Pie chart\n",
    "colors = plt.cm.Set3(range(len(dist_df)))\n",
    "axes[1].pie(dist_df['Jumlah'], labels=dist_df['Kelas'], autopct='%1.1f%%', \n",
    "            colors=colors, startangle=90)\n",
    "axes[1].set_title('Proporsi Kelas (%)', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('class_distribution_after_sampling.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Plot distribusi kelas disimpan: class_distribution_after_sampling.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151543a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistik Deskriptif Data Setelah Sampling\n",
    "print(\"=\"*60)\n",
    "print(\"STATISTIK DESKRIPTIF - DATA NUMERIK\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Ambil kolom numerik dari train_sampled (sebelum feature engineering)\n",
    "numeric_cols = train_sampled.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Exclude kolom 'is_train' jika ada\n",
    "if 'is_train' in numeric_cols:\n",
    "    numeric_cols.remove('is_train')\n",
    "\n",
    "# Statistik deskriptif\n",
    "stats_desc = train_sampled[numeric_cols].describe().T\n",
    "stats_desc['missing'] = train_sampled[numeric_cols].isnull().sum()\n",
    "stats_desc['missing_pct'] = (stats_desc['missing'] / len(train_sampled) * 100).round(2)\n",
    "\n",
    "# Reorder columns\n",
    "stats_desc = stats_desc[['count', 'missing', 'missing_pct', 'mean', 'std', 'min', '25%', '50%', '75%', 'max']]\n",
    "\n",
    "print(stats_desc.to_string())\n",
    "\n",
    "# Simpan ke CSV\n",
    "stats_desc.to_csv('statistics_after_sampling.csv')\n",
    "print(f\"\\n‚úÖ Statistik deskriptif disimpan: statistics_after_sampling.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06484861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisasi distribusi fitur-fitur penting\n",
    "print(\"=\"*60)\n",
    "print(\"VISUALISASI DISTRIBUSI FITUR NUMERIK\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Pilih beberapa fitur penting untuk divisualisasikan\n",
    "important_features = [\n",
    "    'Distance_KM', 'Duration_Minutes', 'Est_Price_IDR', \n",
    "    'GPS_Accuracy_M', 'Battery_Level', 'Accel_X', 'Accel_Y', 'Accel_Z'\n",
    "]\n",
    "\n",
    "# Filter hanya yang ada di data\n",
    "available_features = [f for f in important_features if f in train_sampled.columns]\n",
    "\n",
    "if len(available_features) > 0:\n",
    "    # Hitung jumlah baris yang dibutuhkan\n",
    "    n_cols = 3\n",
    "    n_rows = (len(available_features) + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows*4))\n",
    "    axes = axes.flatten() if n_rows > 1 else [axes] if n_cols == 1 else axes\n",
    "    \n",
    "    for idx, feature in enumerate(available_features):\n",
    "        train_sampled[feature].hist(bins=50, ax=axes[idx], color='skyblue', edgecolor='black', alpha=0.7)\n",
    "        axes[idx].set_title(f'Distribusi {feature}', fontsize=10, fontweight='bold')\n",
    "        axes[idx].set_xlabel(feature, fontsize=9)\n",
    "        axes[idx].set_ylabel('Frekuensi', fontsize=9)\n",
    "        axes[idx].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for idx in range(len(available_features), len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('feature_distributions_after_sampling.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"‚úÖ Plot distribusi fitur disimpan: feature_distributions_after_sampling.png\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Tidak ada fitur numerik yang tersedia untuk divisualisasikan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af776cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simpan data setelah sampling ke CSV untuk analisis lebih lanjut\n",
    "print(\"=\"*60)\n",
    "print(\"EXPORT DATA SETELAH SAMPLING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Gabungkan kembali dengan label untuk export\n",
    "train_sampled_with_label = train_sampled.copy()\n",
    "train_sampled_with_label['Trip_Label'] = train_labels\n",
    "\n",
    "# Simpan ke CSV\n",
    "output_filename = 'train_data_after_sampling.csv'\n",
    "train_sampled_with_label.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"‚úÖ Data training setelah sampling disimpan: {output_filename}\")\n",
    "print(f\"   - Total Rows: {len(train_sampled_with_label):,}\")\n",
    "print(f\"   - Total Columns: {len(train_sampled_with_label.columns)}\")\n",
    "print(f\"   - File Size: {train_sampled_with_label.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Summary ringkas\n",
    "print(\"\\nüìã Summary Kolom:\")\n",
    "print(train_sampled_with_label.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e522fda8",
   "metadata": {},
   "source": [
    "# Swap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f91a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lakukan swap label pada submission yang sudah ada\n",
    "submission['Trip_Label'] = submission['Trip_Label'].replace({'Navigation_Issue': 'Service_Complaint', 'Service_Complaint': 'Navigation_Issue'})\n",
    "\n",
    "# Simpan submission baru\n",
    "submission.to_csv('submission_lgbm_pseudo_swap_all_v3.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
